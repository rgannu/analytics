{
  "name": "analytics-jdbc-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "1",

    "providers": "file",
    "providers.file.class": "org.apache.kafka.common.config.provider.FileConfigProvider",

    "_comment": "target database connection details",
    "connection.url":  "jdbc:postgresql://db:5432/analytics",
    "connection.user": "analytics",
    "connection.password": "analytics",

    "_comment": "unwrapping Debezium’s complex format into a simple one",
    "_comment": "transformation doc: http://kafka.apache.org/documentation.html#connect_transforms",
    "transforms":"dropPrefix, unwrap",
    "transforms.dropPrefix.type":"org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.dropPrefix.regex":"^${file:/secrets/connect-source.properties:database.server.name}.${file:/secrets/connect-source.properties:database.dbname}.(.*)",
    "transforms.dropPrefix.replacement":"$1",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "rewrite",
    "transforms.unwrap.add.fields": "op,table,lsn,source.ts_ms",

    "_comment": "automatically create target tables",
    "auto.create": "true",
    "_comment": "Whether to automatically add columns in the table schema when found to be missing relative to the record schema by issuing ``ALTER``.",
    "auto.evolve": "true",

    "_comment": "insert a row if it does not exist or update an existing one",
    "insert.mode": "upsert",
    "delete.enabled": "true",

    "_comment": "identify the primary key stored in Kafka’s record value field",
    "pk.fields": "id",
    "pk.mode": "record_key",

    "_comment": "topics to consume from",
    "topics.regex": "^${file:/secrets/connect-source.properties:database.server.name}.${file:/secrets/connect-source.properties:database.dbname}.(.*)",

    "_comment": "table name format",
    "_comment": "A format string for the destination table name, which may contain '${topic}' as a placeholder for the originating topic name. For example, ``kafka_${topic}`` for the topic 'orders' will map to the table name 'kafka_orders'.",
    "table.name.format": "${topic}",

    "_comment": "To convert the message (key and value) to AVRO format",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:18081",
    "value.converter.schema.registry.url": "http://schema-registry:18081",
    "key.converter.schemas.enable": "true",
    "value.converter.schemas.enable": "true"
  }
}
